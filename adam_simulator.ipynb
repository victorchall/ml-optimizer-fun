{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Adam Optimizer:\n",
    "\n",
    "A step in Adam is defined by this equation:\n",
    "\n",
    "$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "$\n",
    "\n",
    "- θ are the parameters (weights) of the model.  $t$ denotes the current training step number.\n",
    "\n",
    "(and thus $\\theta_{t+1}$ are the parameters *after* a given training step is applied to the weights, i.e. this equation represents running `optimizer.step()` in PyTorch after a forward and backward pass through the model).​\n",
    "\n",
    "- $\\hat{m}_t$ is the first moment (an exponentially decaying average of past gradients, decaying by beta1).\n",
    "​\n",
    "- $\\hat{v}_t$ is the second moment (an exponentially decaying average of past squared gradients, decaying by beta2).\n",
    "\n",
    "- η is the learning rate. \n",
    "\n",
    "- ϵ (epsilon) is a small number used to prevent division by zero if $\\hat{v}_t$ is zero or so small that it could cause numerical problems based on the precision of the calculation on the computer. (i.e. infinite values or NaNs)\n",
    "\n",
    "TLDR: An simple moving average of gradients calculated, then divided by a exponential moving average (pow2) of your gradients, then multiplied by a learning rate to get your actual gradient update number. That learning step for each weight is then subtracted from the current weights to get the new weights.  This equation is run on every weight that is \"unfrozen\" in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies for this notebook (shouldn't be required on Google Colab)\n",
    "%pip install matplotlib\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and set my_numbers to random (for now)\n",
    "import numpy as np\n",
    "gradients_over_timesteps = np.random.rand(200)\n",
    "print(','.join(str(f\"{x:0.2f}\") for x in gradients_over_timesteps))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run cell once to define the plot function\n",
    "Only need to run once unless you change the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#markdown \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot(beta1:float, beta2:float, epsilon:float, nbeta:float=None, npow:int=None):\n",
    "    m = np.zeros_like(gradients_over_timesteps) # first moment \n",
    "    v = np.zeros_like(gradients_over_timesteps) # second moment\n",
    "    vp = np.zeros_like(gradients_over_timesteps) # q moment \n",
    "\n",
    "    graph_len = len(gradients_over_timesteps)+1\n",
    "\n",
    "    for i in range(1, graph_len):\n",
    "        m[i-1] = beta1 * m[i-2] + (1 - beta1) * gradients_over_timesteps[i-1]  \n",
    "        v[i-1] = beta2 * v[i-2] + (1 - beta2) * (gradients_over_timesteps[i-1]**2)\n",
    "        if nbeta and npow:\n",
    "            vp[i-1] = nbeta * vp[i-2] + (1 - nbeta) * (gradients_over_timesteps[i-1]**npow)\n",
    "\n",
    "    # bias-correction\n",
    "    m_hat = m / (1 - beta1**np.arange(1, graph_len))\n",
    "    v_hat = v / (1 - beta2**np.arange(1, graph_len))\n",
    "    if nbeta and npow:\n",
    "        vp_hat = vp / (1 - nbeta**np.arange(1, graph_len))\n",
    "\n",
    "    # scale moment by n-root\n",
    "    v_hat = np.sqrt(v_hat)\n",
    "\n",
    "    if nbeta and npow: # n-root\n",
    "        vp_hat = np.power(vp_hat,1/npow)\n",
    "\n",
    "    adam_step = m_hat / (v_hat + epsilon)\n",
    "    if nbeta and npow:\n",
    "        adamNpow_step = m_hat / (vp_hat + epsilon)\n",
    "\n",
    "    m_hat_first_poly = np.poly1d(np.polyfit(np.arange(1, graph_len), m_hat, 1))\n",
    "    v_hat_first_poly = np.poly1d(np.polyfit(np.arange(1, graph_len), v_hat, 1))\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(gradients_over_timesteps, label=\"step gradients\")\n",
    "    plt.plot(m_hat, label=\"First Moment with Beta1\")\n",
    "    plt.plot(m_hat_first_poly(np.arange(1, graph_len)), label=\"First Moment trend\")\n",
    "    plt.plot(v_hat, label=\"Second Moment with Beta2\")\n",
    "    plt.plot(v_hat_first_poly(np.arange(1, graph_len)), label=\"Second Moment trend\")\n",
    "    plt.plot(adam_step, label=\"Adam learning step\")\n",
    "    \n",
    "    if nbeta and npow:\n",
    "        plt.plot(vp_hat, label=\"nth moment with nBeta\")\n",
    "    \n",
    "    if nbeta and npow:\n",
    "        plt.plot(adamNpow_step, label=\"Adam with nth moment denominator, learning Step\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"training step\")\n",
    "    plt.title(\"Adam simulator with given gradients\")\n",
    "    plt.show()\n",
    "    # return an image of the plot\n",
    "    return plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run **one** of the following cells to set the gradients_over_timesteps list\n",
    "\n",
    "The fake gradients are set to random in a cell above, but you can try one of these patterns out if you want to see how the optimizer behaves with different gradients.\n",
    "\n",
    "Feel free to play with different values here.  This list represents a gradient value for each training step.  We are only \"simulating\" a model with a single weight and many timesteps. \n",
    "\n",
    "In a real model, the gradients would be calculated by the model during each training step and you would have different gradients for every unfrozen weight in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL a specific pattern\n",
    "gradients_over_timesteps = [\n",
    "    # high frequency pattern\n",
    "    1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,\n",
    "    1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,\n",
    "    1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,\n",
    "    # lower frequency pattern\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.7,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,0.5,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1,0.5,0.5,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,0.3,\n",
    "    0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05,0.15,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,0.1,\n",
    "    # high frequency, low amplitude pattern centered on zero\n",
    "    -0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,\n",
    "    -0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,\n",
    "    -0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,-0.001,0.001,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL a sine wave\n",
    "steps = 300\n",
    "gradients_over_timesteps = np.sin(np.linspace(0, 20*np.pi, steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIONAL yet another pattern to try for fun\n",
    "gradients_over_timesteps = [\n",
    "    0.4,0.0,0.3,0.0,0.3,0.0,0.4,0.0,0.3,0.0,0.3,0.0,0.4,0.0,0.3,0.0,0.3,0.0,0.4,0.0,\n",
    "    0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.08,0.0,0.08,0.0,0.11,0.0,0.1,0.0,0.1,0.0,\n",
    "    0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,\n",
    "    0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,0.1,0.0,\n",
    "    0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,0.08,0.0,\n",
    "    0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,0.05,0.0,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak parameters to see the effect on the learning steps\n",
    "You may want to also go back up and try different values for the my_numbers list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8 # probably don't need to change\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "my_plot = plot(beta1, beta2, epsilon)\n",
    "# optionally save\n",
    "#my_plot.figimage(plt.savefig(\"plot.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOY MODEL JUST FOR FUN: Use nth root denominator in place of \"2,sqrt\" moment of normal Adam\n",
    "epsilon = 1e-8 # probably don't need to change\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "# if nbeta = beta2 and npower=2, this is equivalent to Adam\n",
    "# you can set npower=2 and just change nbeta to see what affect changing beta2 would have\n",
    "nbeta = 0.99\n",
    "npower = 4 # INT: use nth power and nth root in place of \"2,sqrt\" moment of normal Adam\n",
    "my_plot = plot(beta1, beta2, epsilon, nbeta, npower)\n",
    "# optionally save\n",
    "#my_plot.figimage(plt.savefig(\"plot.png\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
